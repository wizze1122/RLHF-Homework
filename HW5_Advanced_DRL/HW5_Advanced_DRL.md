
# 強化學習解決組合最佳化問題 - 筆記

## 1. 緒論

組合最佳化 (CO) 問題的目標是最小化一個負成本函數 ∑T t=1 R(st, at)。本研究介紹並使用一個名為 **RL4CO** 的強化學習基準測試，旨在解決 CO 問題。

## 2. 研究背景

解決 CO 問題的策略分為**建構式策略**（從零開始生成解決方案）和**改進式策略**（精煉現有解決方案）。建構式策略包括自迴歸 (AR) 和非自迴歸 (NAR) 兩種形式。

### 強化學習算法
- **REINFORCE**
- **A2C (Advantage Actor-Critic)**
- **PPO (Proximal Policy Optimization)**

這些算法用於訓練策略網路 π，通過將最大化問題轉換為最小化損失函數來優化。

### 注意力模型（AM）
多數先進的神經組合最佳化方法基於 **Attention 模型 (AM)**，並使用自注意力機制進行編碼器-解碼器架構的訓練。

### AI4CO 社群
AI4CO 是一個開放性研究社群，專注於討論和開發 RL4CO 相關技術。

## 3. 動機

本研究旨在提供一個標準化和可重現的基準測試平台，來評估不同 RL 方法在解決多種 CO 問題中的表現。此外，還評估了 DRL 模型在不同大小和分佈的問題實例上的**泛化能力**。

## 4. 目的

- 建立 **RL4CO** 庫，便於整合可重現和標準化的代碼。
- 廣泛進行基準測試，評估不同模型對 CO 問題的解決效果。
- 比較不同的解碼方案對性能的影響，並評估訓練時未見過的實例（Out-of-Distribution）上的泛化能力。

## 5. 流程

### 策略訓練
- 政策網路 π 的訓練通過優化參數 θ 來最大化累積獎勳，並使用梯度下降進行優化。
- 使用編碼器-解碼器架構進行策略生成，其中編碼器將問題實例映射到嵌入空間，解碼器則基於狀態和嵌入生成動作序列。

### 解碼方案
- 多種解碼方案用於推理階段，如 Greedy、Sampling、Augmentation、Multistart 等。
- **主動搜索**（Active Search）和**高效主動搜索**（Efficient Active Search）是用於改進解決方案的方法。

## 6. 資料集

基準測試使用了多個 CO 問題的實例：

- **FJSSP (彈性作業車間排程問題)**：包括不同大小的實例，如 10×5 和 20×5。
- **VRP 變體 (車輛路徑問題)**：包含多達 16 種變體，測試集包含隨機生成的實例。
- **圖問題**：如設施選址問題（FLP）和最大覆蓋問題（MCP）。

## 7. 方法論

- 強化學習問題表述為生成（建構）或改進解決方案的策略。
- 使用不同的編碼器（如 HGNN、MatNet、MLP、GCN 等）和解碼器（如基於 Pointer 機制的解碼器）進行訓練。
- 訓練過程使用 **REINFORCE (Policy Gradients)**、**A2C** 和 **PPO** 算法，並採用了 **Rollout 基線** 和 **Critic 基線** 來減少梯度變異性。

## 8. 實驗結果

### FJSSP 問題
- 將 **HGNN** 編碼器替換為 **MatNet** 編碼器可將平均完工時間改善約 7%。
- 使用 Pointer 機制替換 MLP 解碼器進一步提高性能，Gap to BKS 降低約 3 倍。
- MatNet 模型表現出較好的泛化能力，超越了 OR-Tools 在較大實例上的結果。

### VRP 變體
- 神經方法在大多數情況下與傳統求解器競爭，特別是在執行速度方面優勢明顯。

### 解碼方案
- Augmentation 解碼方案通常優於其他方案，特別是在少樣本的情況下。

### 參數設置
- 采用了 **混合精度訓練**（FP16-mix）和 **FlashAttention** 來優化計算效率，特別是在處理大型問題實例時。

## 9. 結果圖

以下是訓練前後的比較圖：

### 圖 1

**訓練前**：未訓練的模型，成本約為 22.380  
**訓練後**：訓練過的策略，成本約為 6.509

<img src="../HW5_Advanced_DRL\img\1.png" width="45%"/> 
<br/>
<em>圖 4.1：實例 1 解的可視化對比</em>

### 圖 2

**訓練前**：未訓練的模型，成本約為 23.379  
**訓練後**：訓練過的策略，成本約為 6.409

<img src="../HW5_Advanced_DRL\img\2.png" width="45%"/>
<br/>
<em>圖 4.2：實例 2 解的可視化對比</em>

### 圖 3

**訓練前**：未訓練的模型，成本約為 23.593  
**訓練後**：訓練過的策略，成本約為 6.573

<img src="../HW5_Advanced_DRL\img\3.png" width="45%"/>
<br/>
<em>圖 4.3：實例 3 解的可視化對比</em>

## 10. 總結

- 強化學習方法在解決多種組合最佳化問題中表現良好，特別是在 VRP 和圖問題上，展現出良好的泛化能力。
- 解碼方案和後處理的主動搜索方法可以進一步提高解決方案的品質。
- 高效的訓練和計算優化對於處理大型實例至關重要。

