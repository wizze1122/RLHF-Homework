---
title: ç‘‹å¿—

---

# HW3: Explore and Exploit for Arm-Bandit Problem

---

## 1. Epsilon-Greedy

### ğŸ“Œ (1) æ•¸å­¸å…¬å¼ (LaTeX)

$$
a_t = 
\begin{cases}
\text{random action}, & \text{with probability } \epsilon \\
\arg\max_a Q_t(a), & \text{with probability } 1 - \epsilon
\end{cases}
$$



### ğŸ§  (2) ChatGPT Promptï¼ˆå«æ¼”ç®—æ³•èªªæ˜ï¼‰

è«‹ç”¢ç”Ÿä¸€å€‹ epsilon-greedy çš„ Python ç¨‹å¼ç¢¼ä¾†è§£æ±ºå¤šè‡‚è³­å¾’å•é¡Œã€‚
Epsilon-Greedy æ¼”ç®—æ³•é€éè¨­å®šä¸€å€‹æ¢ç´¢ç‡ Îµï¼Œåœ¨æ¯ä¸€æ¬¡é¸æ“‡ä¸­ä»¥ Îµ çš„æ©Ÿç‡éš¨æ©Ÿæ¢ç´¢ï¼Œå¦å‰‡é¸æ“‡ç›®å‰é ä¼°å ±é…¬æœ€å¤§çš„è‡‚ï¼ˆåˆ©ç”¨ï¼‰ã€‚
è«‹åŒæ™‚ç¹ªè£½æ¯ä¸€è¼ªçš„å¹³å‡å ±é…¬èˆ‡å„è‡‚è¢«é¸æ“‡çš„æ¬¡æ•¸ã€‚


### ğŸ’» (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
```python
import numpy as np
import matplotlib.pyplot as plt

def epsilon_greedy(k=10, steps=1000, epsilon=0.1):
    true_rewards = np.random.normal(0, 1, k)
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    actions = []

    for t in range(steps):
        if np.random.rand() < epsilon:
            a = np.random.randint(k)
        else:
            a = np.argmax(Q)
        r = np.random.normal(true_rewards[a], 1)
        N[a] += 1
        Q[a] += (r - Q[a]) / N[a]
        rewards.append(r)
        actions.append(a)

    return rewards, actions

rewards, actions = epsilon_greedy()
average_rewards = np.cumsum(rewards) / (np.arange(len(rewards)) + 1)

plt.plot(average_rewards)
plt.title("Epsilon-Greedy: Average Reward")
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.show()

plt.hist(actions, bins=np.arange(11)-0.5, edgecolor='black')
plt.title("Epsilon-Greedy: Action Count")
plt.xlabel("Action")
plt.ylabel("Count")
plt.show()
```
<img src="./IMG/greedy1.png" alt="åˆå§‹ç•«é¢" width="300"/>
<img src="./IMG/greedy2.png" alt="åˆå§‹ç•«é¢" width="300"/>

### ğŸ§¾ (4) çµæœèªªæ˜
- é©åº¦æ¢ç´¢ï¼ˆÎµ=0.1ï¼‰å¯å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ï¼›
- æœ€å¸¸é¸æ“‡å ±é…¬è¼ƒé«˜çš„æ‹‰éœ¸è‡‚ï¼›
- åœ–è¡¨é¡¯ç¤ºå ±é…¬ç©©å®šæå‡ã€‚

---

## 2. Upper Confidence Bound (UCB)


### ğŸ“Œ (1) æ•¸å­¸å…¬å¼ (LaTeX)
$$
UCB_t(a) = Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}
$$

### ğŸ§  (2) ChatGPT Promptï¼ˆå«æ¼”ç®—æ³•èªªæ˜ï¼‰

è«‹ç”¨ Python æ’°å¯« UCBï¼ˆUpper Confidence Boundï¼‰æ¼”ç®—æ³•ã€‚
UCB æ–¹æ³•å…¼é¡§ç•¶å‰å ±é…¬ä¼°è¨ˆå€¼èˆ‡æ¢ç´¢æ½›åŠ›ï¼Œå°é¸æ“‡æ¬¡æ•¸å°‘çš„è‡‚æœƒçµ¦äºˆæ›´é«˜çš„ç½®ä¿¡ä¸Šé™ï¼Œå¾è€Œé€²è¡Œæ¢ç´¢ã€‚
è«‹ç¹ªè£½å¹³å‡å ±é…¬èˆ‡å„è‡‚é¸æ“‡æ¬¡æ•¸çš„åœ–è¡¨ã€‚


### ğŸ’» (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
```python
import numpy as np
import matplotlib.pyplot as plt
def ucb(k=10, steps=1000, c=2):
    true_rewards = np.random.normal(0, 1, k)
    Q = np.zeros(k)
    N = np.ones(k)
    rewards = []
    actions = []

    for t in range(1, steps+1):
        ucb_values = Q + c * np.sqrt(np.log(t) / N)
        a = np.argmax(ucb_values)
        r = np.random.normal(true_rewards[a], 1)
        N[a] += 1
        Q[a] += (r - Q[a]) / N[a]
        rewards.append(r)
        actions.append(a)

    return rewards, actions

rewards, actions = ucb()
avg_rewards = np.cumsum(rewards) / (np.arange(len(rewards)) + 1)

plt.plot(avg_rewards)
plt.title("UCB: Average Reward")
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.show()

plt.hist(actions, bins=np.arange(11)-0.5, edgecolor='black')
plt.title("UCB: Action Count")
plt.xlabel("Action")
plt.ylabel("Count")
plt.show()
```
<img src="./IMG/UCB1.png" alt="åˆå§‹ç•«é¢" width="300"/>
<img src="./IMG/UCB2.png" alt="åˆå§‹ç•«é¢" width="300"/>

### ğŸ§¾ (4) çµæœèªªæ˜
- åˆæœŸæ¢ç´¢è¼ƒå¤šï¼Œå¾ŒæœŸè¿…é€Ÿæ”¶æ–‚ï¼›
- æ›´å¿«å®šä½é«˜å ±é…¬è‡‚ï¼›
- é©åˆè³‡è¨Šä¸å°ç¨±æˆ–éœ€ç©æ¥µæ¢ç´¢çš„æƒ…æ³ã€‚

---

## 3. Softmax

### ğŸ“Œ (1) æ•¸å­¸å…¬å¼ (LaTeX)
$$
P(a) = \frac{e^{Q(a)/\tau}}{\sum_b e^{Q(b)/\tau}}
$$

### ğŸ§  (2) ChatGPT Promptï¼ˆå«æ¼”ç®—æ³•èªªæ˜ï¼‰

è«‹æ’°å¯« Softmax MAB ç­–ç•¥çš„ Python ç¨‹å¼ã€‚
Softmax é€éæº«åº¦åƒæ•¸ Ï„ æ§åˆ¶æ¯å€‹å‹•ä½œè¢«é¸ä¸­çš„æ©Ÿç‡ï¼ŒÏ„ è¶Šå°è¶Šåå‘ exploitationï¼ŒÏ„ è¶Šå¤§è¶Šéš¨æ©Ÿã€‚
è«‹å±•ç¤ºä¸åŒ Ï„ å€¼ä¸‹çš„è¡¨ç¾å·®ç•°ã€‚


### ğŸ’» (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
```python
import numpy as np
import matplotlib.pyplot as plt
def softmax(Q, tau):
    exp_Q = np.exp(Q / tau)
    return exp_Q / np.sum(exp_Q)

def softmax_bandit(k=10, steps=1000, tau=0.5):
    true_rewards = np.random.normal(0, 1, k)
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    actions = []

    for _ in range(steps):
        probs = softmax(Q, tau)
        a = np.random.choice(k, p=probs)
        r = np.random.normal(true_rewards[a], 1)
        N[a] += 1
        Q[a] += (r - Q[a]) / N[a]
        rewards.append(r)
        actions.append(a)

    return rewards, actions

rewards, actions = softmax_bandit()
avg_rewards = np.cumsum(rewards) / (np.arange(len(rewards)) + 1)

plt.plot(avg_rewards)
plt.title("Softmax: Average Reward")
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.show()

plt.hist(actions, bins=np.arange(11)-0.5, edgecolor='black')
plt.title("Softmax: Action Count")
plt.xlabel("Action")
plt.ylabel("Count")
plt.show()
```
<img src="./IMG/softmax1.png" alt="åˆå§‹ç•«é¢" width="300"/>
<img src="./IMG/softmax2.png" alt="åˆå§‹ç•«é¢" width="300"/>

### ğŸ§¾ (4) çµæœèªªæ˜
- Ï„ æ§åˆ¶æ¢ç´¢ç¨‹åº¦ï¼Œé«˜ Ï„ å°è‡´å¹³å‡å ±é…¬æ”¶æ–‚æ…¢ï¼›
- è¼ƒç©©å®šæ¢ç´¢æ–¹å¼ï¼Œé©åˆå˜—è©¦å¤šç¨®è‡‚çš„å ´æ™¯ï¼›
- å¯æ­é…å‹•æ…‹ Ï„ æ”¹å–„æ—©æœŸå­¸ç¿’ã€‚

---

## 4. Thompson Sampling

### ğŸ“Œ (1) æ•¸å­¸å…¬å¼ (LaTeX)
$$
\theta_a \sim \text{Beta}(\alpha_a, \beta_a), \\
a_t = \arg\max_a \theta_a
$$

### ğŸ§  (2) ChatGPT Promptï¼ˆå«æ¼”ç®—æ³•èªªæ˜ï¼‰

è«‹æ’°å¯« Thompson Sampling çš„ Python ç¨‹å¼ã€‚
æ­¤æ¼”ç®—æ³•ç‚ºè²è‘‰æ–¯æ–¹æ³•ï¼Œå°æ¯å€‹å‹•ä½œçš„æˆåŠŸæ©Ÿç‡å»ºæ¨¡ç‚º Beta åˆ†å¸ƒï¼Œä¸¦å¾å¾Œé©—åˆ†å¸ƒä¸­æŠ½æ¨£ä¾†åšé¸æ“‡ã€‚
è«‹ç•«å‡ºæ¯è‡‚è¢«é¸æ“‡æ¬¡æ•¸èˆ‡å¹³å‡å ±é…¬ã€‚


### ğŸ’» (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
```python
import numpy as np
import matplotlib.pyplot as plt
def thompson_sampling(k=10, steps=1000):
    true_probs = np.random.rand(k)
    alpha = np.ones(k)
    beta_vals = np.ones(k)
    rewards = []
    actions = []

    for _ in range(steps):
        theta = np.random.beta(alpha, beta_vals)
        a = np.argmax(theta)
        reward = np.random.rand() < true_probs[a]
        alpha[a] += reward
        beta_vals[a] += 1 - reward
        rewards.append(reward)
        actions.append(a)

    return rewards, actions

rewards, actions = thompson_sampling()
avg_rewards = np.cumsum(rewards) / (np.arange(len(rewards)) + 1)

plt.plot(avg_rewards)
plt.title("Thompson Sampling: Average Reward")
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.show()

plt.hist(actions, bins=np.arange(11)-0.5, edgecolor='black')
plt.title("Thompson Sampling: Action Count")
plt.xlabel("Action")
plt.ylabel("Count")
plt.show()
```

<img src="./IMG/Thompson Sampling1.png" alt="åˆå§‹ç•«é¢" width="300"/>
<img src="./IMG/Thompson Sampling2.png" alt="åˆå§‹ç•«é¢" width="300"/>

### ğŸ§¾ (4) çµæœèªªæ˜
- èƒ½å¿«é€Ÿæ”¶æ–‚è‡³æˆåŠŸæ©Ÿç‡é«˜çš„è‡‚ï¼›
- æˆæœ¬ä½ã€æ•ˆç‡é«˜ï¼Œé©åˆæ‡‰ç”¨åœ¨é»æ“Šé æ¸¬ã€æ¨è–¦ç³»çµ±ç­‰ï¼›
- å°åˆå§‹å…ˆé©—è¨­å®šè¼ƒæ•æ„Ÿï¼Œä½†å¹³å‡è¡¨ç¾å„ªç•°ã€‚

---

